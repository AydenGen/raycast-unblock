[General]
# If you deploy this service on a remote server,
# it is recommended to set mode to "remote", maybe one day this program will be used
# and you **should** set host to "0.0.0.0", or you can't access the service from the outside
mode = "local"
port = 3000
host = "0.0.0.0"
# If there are some problems, you can set debug & logger to true
debug = false
# Fastify Logger
logger = false
# If you want the service to listen to the changes of the configuration file and update automatically,
# you can set watch to true
watch = false
# The location of the sync file, default is "icloud", and you can also set it to "local"
# # The iCloud storage solution is only effective when deployed on the macOS client
# # **iCloud storage solution** is the *default* option in *macOS* deployments
sync = "icloud" # icloud / local

[General.Https]
# If you want to use HTTPS, you can set the following configuration
# enabled = true
# # You can specify the host to the certificate file (auto generate mode)
# host = '192.168.3.2'
# # You can also specify the path to your existing certificate file
# key = "path
# cert = "path"
# ca = "path"

[AI]
default = "openai"
# If the parameter is not set in the specific AI service,
# this value will be used
# For example:
#    If I don't set the temperature parameter in AI.OpenAI, this value will be used
#    But if I set the temperature parameter in AI.Gemini, the temperature parameter in AI.Gemini will be used
# temperature = 0.5
# max_tokens = 100

[AI.OpenAI]
# If the default model is not set,
# or...
# if the default model is set,
# but the specific AI service's model is not set,
# the default model written in the code will be used
# default = "gpt-3.5-turbo-16k.legacy"
# You can edit the base_url if you want to use the custom OpenAI server
# base_url = "https://api.openai.com/v1"
api_key = "YOUR_API_KEY"

# if you'd like to use azure openai
# is_azure = true
# base_url = "https://<resource_name>.openai.azure.com"
# azure_deployment_name = "YOUR_AZURE_DEPLOYMENT_ID" # if not provided, use req.body.model

# If the parameter is set in the specific AI service
# this value will be used, and it has the highest priority
# temperature = 0.5
# max_tokens = 100

# Custom OpenAI Model
# You can add your own OpenAI model just like the following:
# # [NOTICE] You shouldn't use the dot in the model name. It will be parsed as a section
# [AI.OpenAI.Models.gpt-3_5-turbo-16k-legacy]
# id = "gpt-3.5-turbo-16k.legacy" # if it's not provided, it will be generated from the Object key. For example, here it will be "gpt-3.5-turbo-16k-legacy"
# model = "gpt-3.5-turbo-16k.legacy" # if it's not provided, it will be generated from the Object key.
# name = "Legacy GPT-3.5 Turbo" # if it's not provided, it will be generated from the Object key.

[AI.Gemini]
api_key = ""
# temperature = 0.5
# max_tokens = 100

[Translate]
# You can choose the default translation service from the following:
# shortcut, deeplx, ai, libretranslate, google
# Default: deeplx
default = "deeplx"

# Maybe one day there will be a [Translate.Shortcuts] configuration here...
# [Translate.Shortcuts]

[Translate.DeepLX]
# proxy_endpoint = ""
# access_token = ""

[Translate.AI]
# If the default model is not set,
# or...
# if the default model is set,
# but the specific AI service's model is not set,
# the default model written in the code will be used
# Default: openai
default = "openai"
# The model used by the AI service
# (only effective for openai)
# Default: gpt-3.5-turbo
model = "gpt-3.5-turbo"

[Translate.LibreTranslate]
base_url = "https://libretranslate.com"
# You can choose the type from the following:
# reserve, api
# Default: reserve
type = "reserve"
# If you choose api, you should set the api_key
api_key = ""
# The following is for the legacy configuration
# They will be removed in the future

# They are for the legacy configuration
# YOU SHOULD NOT USE THEM
# THEY WILL CAUSE WARNING IN LOGS
# [Legacy]
# ai_type = "OpenAI"
# ai_api_key = ""
# openai_base_url = "https://api.openai.com"
# ai_max_tokens = 100
# ai_temperature = 0.5
